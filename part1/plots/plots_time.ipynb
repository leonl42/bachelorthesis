{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 16:37:15.264569: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-01-24 16:37:15.276233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-01-24 16:37:15.289716: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 37.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean step without norm 0.021856519085678025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 37.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean step with norm 0.02315261735388981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/miri/Documents/bachelorthesis/part1\")\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.5\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import argparse\n",
    "from jax.sharding import Mesh, PartitionSpec as P,NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "settings = {\"num_devices\": 1, \"num_experiments_per_device\": 3, \"random_key\": 42, \"num_steps\": 1000, \"save_args\": {\"save_states_every\": 5000, \"save_train_stats_every\": 1000, \"save_test_stats_every\": 1000, \"save_grad_every\": 5000, \"save_hessian_every\": -1}, \"model\": {\"model\": \"vgg11\", \"num_classes\": 10, \"activation_fn\": \"relu\"}, \"dataset\": {\"dataset\": \"cifar10\", \"batch_size\": 128, \"dataset_path\": \"../datasets/\"}, \"optimizer\": {\"optimizer\": \"sgdm\", \"lr\": 0.001, \"lambda_wd\": 0, \"momentum\": 0.9, \"apply_wd_every\": 1}, \"norm\": {\"change_scale\": \"identity\", \"norm_fn\": \"global_center_std_uncenter\", \"norm_multiply\": 0.2, \"norm_every\": 1, \"reverse_norms\": False}, \"at_step\": 0}\n",
    "args = dict_to_namespace(settings)\n",
    "# Get the cpu device in order to put all params on the cpu before distributing them to the other devices (either cpu's or gpu's)\n",
    "default_cpu_device = jax.devices(\"cpu\")[0]\n",
    "\n",
    "# Create device mesh in order to distribute params on devices\n",
    "devices = mesh_utils.create_device_mesh((args.num_devices,))\n",
    "mesh = Mesh(devices, axis_names=('d',))\n",
    "named_sharding = NamedSharding(mesh, P('d'))\n",
    "\n",
    "# Initialize the random seed\n",
    "split_key = jax.random.key(args.random_key)\n",
    "\n",
    "ds_train,ds_test = get_dataset(args,False)\n",
    "\n",
    "model,layer_depth_dict,num_layers = get_model(args)\n",
    "\n",
    "# Get key paths of all layers\n",
    "with jax.default_device(default_cpu_device):\n",
    "    helper_weights = model.init(jax.random.key(0),jnp.ones((1,32,32,3)))[\"params\"]\n",
    "\n",
    "optimizer = get_optimizer(args,helper_weights=helper_weights)\n",
    "\n",
    "\n",
    "# Initialize model \n",
    "if args.at_step == 0:\n",
    "    keys = jax.random.split(split_key,num=args.num_devices*args.num_experiments_per_device+1)\n",
    "    sk,split_key = keys[:-1],keys[-1]\n",
    "    weights,batch_stats,optimizer_state = get_states(model.init,optimizer.init,jnp.ones((1,32,32,3)),sk,default_cpu_device)\n",
    "\n",
    "weights,batch_stats,optimizer_state = device_put(named_sharding,weights,batch_stats,optimizer_state)\n",
    "\n",
    "if args.norm.norm_fn == \"center_std_uncenter\" or args.norm.norm_fn == \"global_center_std_uncenter\":\n",
    "    if args.at_step == 0:\n",
    "        std_weights = weights\n",
    "\n",
    "# If we want to use the normalization scheme proposed by Niehaus et al. 2024, we have to calculate the standard deviation before training\n",
    "if args.norm.norm_fn == \"center_std_uncenter\":\n",
    "    # Get the standard deviations of the weights in the beginning\n",
    "    target_std = tree_map_with_path(lambda s,w : jax.vmap(lambda x : jnp.std(x,axis=tuple(range(len(x.shape)-1)),keepdims=True),in_axes=(0,))(w) if substrings_in_path(s,\"conv\",\"kernel\") else None, std_weights)\n",
    "    # Function that applies settings.norm_fn to every leaf of the params dictionary\n",
    "    # The result is a dictionary that contains the normed params\n",
    "    norm_fn =  jax.jit(lambda tree : tree_map_with_path(lambda s,w,std : get_norm_fn(args.norm.norm_fn)(w,args.norm.norm_multiply,std) if substrings_in_path(s,\"conv\",\"kernel\") else w,tree,target_std))\n",
    "elif args.norm.norm_fn == \"global_center_std_uncenter\":\n",
    "    # Get the standard deviations of the weights in the beginning\n",
    "    target_std = tree_map_with_path(lambda s,w : jax.vmap(lambda x : jnp.std(x,keepdims=True),in_axes=(0,))(w) if substrings_in_path(s,\"conv\",\"kernel\") else None, std_weights)\n",
    "    # Function that applies settings.norm_fn to every leaf of the params dictionary\n",
    "    # The result is a dictionary that contains the normed params\n",
    "    norm_fn =  jax.jit(lambda tree : tree_map_with_path(lambda s,w,std : get_norm_fn(args.norm.norm_fn)(w,args.norm.norm_multiply,std) if substrings_in_path(s,\"conv\",\"kernel\") else w,tree,target_std))\n",
    "else:\n",
    "    # Function that applies settings.norm_fn to every leaf of the params dictionary\n",
    "    # The result is a dictionary that contains the normed params\n",
    "    norm_fn =  jax.jit(lambda tree : tree_map_with_path(lambda s,w : get_norm_fn(args.norm.norm_fn)(w,args.norm.norm_multiply) if substrings_in_path(s,\"conv\",\"kernel\") else w,tree))\n",
    "\n",
    "# We want to be able to specify how much the weights are changed via:\n",
    "# new_params = (1-change_scale)*params + change_scale*params_normed\n",
    "# If change_scale is not provided via settings, we simply set it to 1. Otherwise change scale is a function that takes:\n",
    "# n -> current step\n",
    "# N -> Max steps\n",
    "# l -> current layer\n",
    "# L -> Max layers\n",
    "change_scale = get_change_scale(args.norm.change_scale)\n",
    "\n",
    "# This function calculates the new params as described earlier\n",
    "def change_fn(w,normed_w,n,N,l,L):\n",
    "    s = change_scale(n,N,l,L)\n",
    "    return (1-s)*w + s*normed_w\n",
    "\n",
    "# This function takes as input the params, the normed params, n, N, the dictionary containing the layer depth and L\n",
    "# change_fn is then applied to every common leaf of params, normed_params and the layer depth dictionary \n",
    "layerwise_stepscale_fn = jax.jit(lambda params,normed_params,n,N,layer_depth_dict,L : \n",
    "                                    tree_map_with_path(lambda s,w,normed_w,l : change_fn(w,normed_w,n,N,l,L) \n",
    "                                                    if substrings_in_path(s,\"conv\",\"kernel\") else w,params,normed_params,layer_depth_dict))\n",
    "\n",
    "import time\n",
    "times_step = []\n",
    "for i,(img,lbl) in zip(tqdm(range(1,args.num_steps+1)),ds_train):\n",
    "\n",
    "    # Generate new random keys for this step\n",
    "    keys = jax.random.split(split_key,num=args.num_devices*args.num_experiments_per_device+1)\n",
    "    sk,split_key = keys[:-1],keys[-1]\n",
    "    \n",
    "    start = time.time()\n",
    "    #if args.norm.start_after is None or i>=args.norm.start_after:\n",
    "    #    if args.norm.stop_after is None or i<=args.norm.stop_after:\n",
    "    #        if i%args.norm.norm_every == 0 and args.norm.norm_every != -1:\n",
    "    #            \n",
    "    #            weights = layerwise_stepscale_fn(weights,norm_fn(weights),i,args.num_steps,layer_depth_dict,num_layers)\n",
    "                #tree_map(lambda x : x.block_until_ready(), weights)\n",
    "                #end  = time.time()\n",
    "                #if i> 5:\n",
    "                #    times_norm.append(end-start)\n",
    "\n",
    "    if i%args.optimizer.apply_wd_every == 0 and args.optimizer.apply_wd_every != -1:\n",
    "        pass\n",
    "\n",
    "    grad,aux = get_grad_fn(weights,batch_stats,img,lbl,sk,model.apply)\n",
    "    batch_stats = aux[\"batch_stats\"]\n",
    "    weights,optimizer_state = update_states_fn(weights, grad, optimizer_state, optimizer.update)\n",
    "    tree_map(lambda x : x.block_until_ready(), weights),tree_map(lambda x : x.block_until_ready(), optimizer_state)\n",
    "    end  = time.time()\n",
    "    if i> 5:\n",
    "        times_step.append(end-start)\n",
    "\n",
    "print(\"Mean step without norm {0}\".format(sum(times_step)/len(times_step)))\n",
    "\n",
    "\n",
    "times_step = []\n",
    "for i,(img,lbl) in zip(tqdm(range(1,args.num_steps+1)),ds_train):\n",
    "\n",
    "    # Generate new random keys for this step\n",
    "    keys = jax.random.split(split_key,num=args.num_devices*args.num_experiments_per_device+1)\n",
    "    sk,split_key = keys[:-1],keys[-1]\n",
    "    \n",
    "    start = time.time()\n",
    "    if args.norm.start_after is None or i>=args.norm.start_after:\n",
    "        if args.norm.stop_after is None or i<=args.norm.stop_after:\n",
    "            if i%args.norm.norm_every == 0 and args.norm.norm_every != -1:\n",
    "                \n",
    "                weights = layerwise_stepscale_fn(weights,norm_fn(weights),i,args.num_steps,layer_depth_dict,num_layers)\n",
    "\n",
    "\n",
    "    if i%args.optimizer.apply_wd_every == 0 and args.optimizer.apply_wd_every != -1:\n",
    "        pass\n",
    "\n",
    "    grad,aux = get_grad_fn(weights,batch_stats,img,lbl,sk,model.apply)\n",
    "    batch_stats = aux[\"batch_stats\"]\n",
    "    weights,optimizer_state = update_states_fn(weights, grad, optimizer_state, optimizer.update)\n",
    "    tree_map(lambda x : x.block_until_ready(), weights),tree_map(lambda x : x.block_until_ready(), optimizer_state)\n",
    "    end  = time.time()\n",
    "    if i> 5:\n",
    "        times_step.append(end-start)\n",
    "\n",
    "\n",
    "print(\"Mean step with norm {0}\".format(sum(times_step)/len(times_step)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emecomspec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
